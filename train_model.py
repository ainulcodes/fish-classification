"""
Script untuk training model CNN klasifikasi ikan air tawar
Ikan yang didukung: Lele, Patin, Nila, Gurame

Cara menggunakan:
1. Siapkan dataset di folder dataset/train/ dan dataset/validation/
2. Jalankan: python train_model.py
3. Model akan disimpan di backend/models/fish_classifier.h5
"""

import os
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from datetime import datetime

# TensorFlow dan Keras
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.applications import MobileNetV2

# Konfigurasi
IMG_SIZE = (224, 224)
BATCH_SIZE = 32  # Will be adjusted based on dataset size
EPOCHS = 50
NUM_CLASSES = 4  # Lele, Patin, Nila, Gurame

# Paths
ROOT_DIR = Path(__file__).parent
DATASET_DIR = ROOT_DIR / 'dataset'
TRAIN_DIR = DATASET_DIR / 'train'
VAL_DIR = DATASET_DIR / 'validation'
MODEL_DIR = ROOT_DIR / 'backend' / 'models'
MODEL_PATH = MODEL_DIR / 'fish_classifier.h5'

# Create models directory if not exists
MODEL_DIR.mkdir(parents=True, exist_ok=True)

def check_dataset():
    """Cek apakah dataset tersedia"""
    print("\n" + "="*60)
    print("CHECKING DATASET")
    print("="*60)

    if not TRAIN_DIR.exists():
        print(f"‚ùå ERROR: Training directory tidak ditemukan: {TRAIN_DIR}")
        print("\nSilakan buat folder dataset dengan struktur:")
        print("dataset/")
        print("  ‚îú‚îÄ‚îÄ train/")
        print("  ‚îÇ   ‚îú‚îÄ‚îÄ Lele/")
        print("  ‚îÇ   ‚îú‚îÄ‚îÄ Patin/")
        print("  ‚îÇ   ‚îú‚îÄ‚îÄ Nila/")
        print("  ‚îÇ   ‚îî‚îÄ‚îÄ Gurame/")
        print("  ‚îî‚îÄ‚îÄ validation/")
        print("      ‚îú‚îÄ‚îÄ Lele/")
        print("      ‚îú‚îÄ‚îÄ Patin/")
        print("      ‚îú‚îÄ‚îÄ Nila/")
        print("      ‚îî‚îÄ‚îÄ Gurame/")
        return False

    classes = ['Lele', 'Patin', 'Nila', 'Gurame']

    print("\nDataset Training:")
    train_counts = {}
    for cls in classes:
        class_dir = TRAIN_DIR / cls
        if class_dir.exists():
            count = len(list(class_dir.glob('*.jpg'))) + len(list(class_dir.glob('*.png'))) + len(list(class_dir.glob('*.jpeg')))
            train_counts[cls] = count
            print(f"  {cls}: {count} gambar")
        else:
            print(f"  {cls}: ‚ùå Folder tidak ditemukan")
            train_counts[cls] = 0

    print("\nDataset Validation:")
    val_counts = {}
    for cls in classes:
        class_dir = VAL_DIR / cls
        if class_dir.exists():
            count = len(list(class_dir.glob('*.jpg'))) + len(list(class_dir.glob('*.png'))) + len(list(class_dir.glob('*.jpeg')))
            val_counts[cls] = count
            print(f"  {cls}: {count} gambar")
        else:
            print(f"  {cls}: ‚ùå Folder tidak ditemukan")
            val_counts[cls] = 0

    # Check if we have enough data
    total_train = sum(train_counts.values())
    total_val = sum(val_counts.values())

    print(f"\nTotal Training: {total_train}")
    print(f"Total Validation: {total_val}")

    if total_train == 0:
        print("\n‚ùå ERROR: Tidak ada data training!")
        print("Silakan tambahkan gambar ikan ke folder dataset/train/")
        return False

    # Warning for minimal dataset
    if total_train < 40:  # Less than 10 images per class
        print("\n‚ùå ERROR: Data terlalu sedikit!")
        print(f"Minimal 10 gambar per kelas (40 total), saat ini: {total_train}")
        return False
    elif total_train < 200:  # Less than 50 images per class
        print("\n‚ö†Ô∏è  WARNING: Dataset Minimal!")
        print(f"Total gambar: {total_train}")
        print("Dengan dataset minimal (10-50 per kelas):")
        print("  - Akurasi yang diharapkan: 60-75%")
        print("  - Model mungkin overfitting")
        print("  - Akan menggunakan data augmentation agresif")
        print("  - Disarankan tambah data bertahap untuk hasil lebih baik")
        print("\nUntuk hasil optimal (85-95% akurasi):")
        print("  - Minimal: 100-200 gambar per kelas")
        print("  - Ideal: 500-1000 gambar per kelas")

    if total_val == 0:
        print("\n‚ö†Ô∏è  INFO: Tidak ada data validation terpisah")
        print("Training akan menggunakan 20% dari training data untuk validation")

    return True

def create_data_generators():
    """Buat data generators untuk training dan validation"""
    print("\n" + "="*60)
    print("CREATING DATA GENERATORS")
    print("="*60)

    # Count total images to determine augmentation level
    total_images = 0
    for cls in ['Lele', 'Patin', 'Nila', 'Gurame']:
        class_dir = TRAIN_DIR / cls
        if class_dir.exists():
            total_images += len(list(class_dir.glob('*.jpg'))) + \
                           len(list(class_dir.glob('*.png'))) + \
                           len(list(class_dir.glob('*.jpeg')))

    # Use aggressive augmentation for small datasets
    if total_images < 200:  # Minimal dataset (10-50 per class)
        print("üìä Dataset Mode: MINIMAL (using aggressive augmentation)")
        train_datagen = ImageDataGenerator(
            rescale=1./255,
            rotation_range=40,           # Increased from 20
            width_shift_range=0.3,       # Increased from 0.2
            height_shift_range=0.3,      # Increased from 0.2
            shear_range=0.3,             # Increased from 0.2
            zoom_range=0.3,              # Increased from 0.2
            horizontal_flip=True,
            vertical_flip=True,          # Added
            brightness_range=[0.7, 1.3], # Added
            fill_mode='nearest',
            validation_split=0.2
        )
    else:  # Normal dataset
        print("üìä Dataset Mode: NORMAL (standard augmentation)")
        train_datagen = ImageDataGenerator(
            rescale=1./255,
            rotation_range=20,
            width_shift_range=0.2,
            height_shift_range=0.2,
            shear_range=0.2,
            zoom_range=0.2,
            horizontal_flip=True,
            fill_mode='nearest',
            validation_split=0.2
        )

    # Hanya rescaling untuk validation
    val_datagen = ImageDataGenerator(rescale=1./255)

    # Adjust batch size for small datasets
    batch_size = BATCH_SIZE
    if total_images < 200:
        batch_size = min(8, total_images // 8)  # Smaller batch for small datasets
        print(f"Adjusted batch size: {batch_size} (optimal for small dataset)")

    # Training generator
    train_generator = train_datagen.flow_from_directory(
        TRAIN_DIR,
        target_size=IMG_SIZE,
        batch_size=batch_size,
        class_mode='categorical',
        subset='training' if not VAL_DIR.exists() else None
    )

    # Validation generator
    if VAL_DIR.exists() and len(list(VAL_DIR.glob('*/*.jpg'))) > 0:
        print("Using separate validation directory")
        validation_generator = val_datagen.flow_from_directory(
            VAL_DIR,
            target_size=IMG_SIZE,
            batch_size=batch_size,
            class_mode='categorical'
        )
    else:
        print("Using validation split from training data (20%)")
        validation_generator = train_datagen.flow_from_directory(
            TRAIN_DIR,
            target_size=IMG_SIZE,
            batch_size=batch_size,
            class_mode='categorical',
            subset='validation'
        )

    print(f"\nClasses: {train_generator.class_indices}")
    print(f"Training samples: {train_generator.samples}")
    print(f"Validation samples: {validation_generator.samples}")

    return train_generator, validation_generator

def build_model(use_transfer_learning=True):
    """
    Buat model CNN

    Args:
        use_transfer_learning: Jika True, gunakan MobileNetV2 pre-trained
                              Jika False, buat CNN dari scratch
    """
    print("\n" + "="*60)
    print("BUILDING MODEL")
    print("="*60)

    if use_transfer_learning:
        print("Using Transfer Learning with MobileNetV2")

        # Load MobileNetV2 pre-trained on ImageNet
        base_model = MobileNetV2(
            input_shape=IMG_SIZE + (3,),
            include_top=False,
            weights='imagenet'
        )

        # Freeze base model layers
        base_model.trainable = False

        # Add custom classification head with more dropout for small datasets
        model = keras.Sequential([
            base_model,
            layers.GlobalAveragePooling2D(),
            layers.Dropout(0.5),  # Increased from 0.3 for regularization
            layers.Dense(128, activation='relu'),
            layers.Dropout(0.5),  # Increased from 0.3
            layers.Dense(NUM_CLASSES, activation='softmax')
        ])

    else:
        print("Building CNN from scratch")

        model = keras.Sequential([
            # Block 1
            layers.Conv2D(32, (3, 3), activation='relu', input_shape=IMG_SIZE + (3,)),
            layers.MaxPooling2D((2, 2)),
            layers.BatchNormalization(),

            # Block 2
            layers.Conv2D(64, (3, 3), activation='relu'),
            layers.MaxPooling2D((2, 2)),
            layers.BatchNormalization(),

            # Block 3
            layers.Conv2D(128, (3, 3), activation='relu'),
            layers.MaxPooling2D((2, 2)),
            layers.BatchNormalization(),

            # Block 4
            layers.Conv2D(256, (3, 3), activation='relu'),
            layers.MaxPooling2D((2, 2)),
            layers.BatchNormalization(),

            # Classification head
            layers.Flatten(),
            layers.Dropout(0.5),
            layers.Dense(256, activation='relu'),
            layers.Dropout(0.5),
            layers.Dense(NUM_CLASSES, activation='softmax')
        ])

    # Compile model
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.001),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

    print("\nModel Summary:")
    model.summary()

    return model

def train(model, train_generator, validation_generator):
    """Training model"""
    print("\n" + "="*60)
    print("STARTING TRAINING")
    print("="*60)

    # Callbacks
    callbacks = [
        # Save best model
        ModelCheckpoint(
            str(MODEL_PATH),
            monitor='val_accuracy',
            save_best_only=True,
            verbose=1
        ),

        # Early stopping
        EarlyStopping(
            monitor='val_loss',
            patience=10,
            restore_best_weights=True,
            verbose=1
        ),

        # Reduce learning rate when validation loss plateaus
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=5,
            min_lr=1e-7,
            verbose=1
        )
    ]

    # Train
    history = model.fit(
        train_generator,
        validation_data=validation_generator,
        epochs=EPOCHS,
        callbacks=callbacks,
        verbose=1
    )

    return history

def plot_training_history(history):
    """Plot training history"""
    print("\n" + "="*60)
    print("PLOTTING TRAINING HISTORY")
    print("="*60)

    # Create plots directory
    plots_dir = ROOT_DIR / 'training_plots'
    plots_dir.mkdir(exist_ok=True)

    # Plot accuracy
    plt.figure(figsize=(12, 4))

    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.title('Model Accuracy')
    plt.grid(True)

    # Plot loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.title('Model Loss')
    plt.grid(True)

    # Save plot
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    plot_path = plots_dir / f'training_history_{timestamp}.png'
    plt.tight_layout()
    plt.savefig(plot_path)
    print(f"\nTraining history plot saved to: {plot_path}")

    # Print final metrics
    final_train_acc = history.history['accuracy'][-1]
    final_val_acc = history.history['val_accuracy'][-1]
    final_train_loss = history.history['loss'][-1]
    final_val_loss = history.history['val_loss'][-1]

    print("\n" + "="*60)
    print("TRAINING COMPLETED")
    print("="*60)
    print(f"Final Training Accuracy: {final_train_acc:.4f}")
    print(f"Final Validation Accuracy: {final_val_acc:.4f}")
    print(f"Final Training Loss: {final_train_loss:.4f}")
    print(f"Final Validation Loss: {final_val_loss:.4f}")
    print(f"\nModel saved to: {MODEL_PATH}")

def main():
    """Main training function"""
    print("\n" + "="*60)
    print("FISH CLASSIFIER TRAINING SCRIPT")
    print("="*60)
    print(f"TensorFlow version: {tf.__version__}")
    print(f"GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}")

    # Check dataset
    if not check_dataset():
        print("\n‚ùå Dataset check failed. Exiting...")
        return

    # Ask user if they want to continue
    print("\n" + "="*60)
    response = input("Lanjutkan training? (y/n): ")
    if response.lower() != 'y':
        print("Training dibatalkan.")
        return

    # Ask about transfer learning
    print("\n" + "="*60)
    print("PILIH METODE TRAINING:")
    print("1. Transfer Learning dengan MobileNetV2 (Recommended - lebih cepat & akurat)")
    print("2. CNN from Scratch (Membutuhkan lebih banyak data)")
    choice = input("Pilih (1/2): ")
    use_transfer_learning = (choice == '1')

    # Create data generators
    train_gen, val_gen = create_data_generators()

    # Build model
    model = build_model(use_transfer_learning=use_transfer_learning)

    # Train model
    history = train(model, train_gen, val_gen)

    # Plot results
    plot_training_history(history)

    print("\n" + "="*60)
    print("‚úì TRAINING SELESAI!")
    print("="*60)
    print(f"\nModel telah disimpan di: {MODEL_PATH}")
    print("\nUntuk menggunakan model:")
    print("1. Restart backend server")
    print("2. Backend akan otomatis load model yang baru")
    print("3. Upload gambar ikan untuk klasifikasi")
    print("\n" + "="*60)

if __name__ == '__main__':
    main()
